[["index.html", "Bayesian Linear Regression Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem 1.2 Normal Inverse-Gamma (NIG) prior 1.3 Posterior distribution 1.4 Bayesian prediction 1.5 Sampling process", " Bayesian Linear Regression authors 2023-01-22 Chapter 1 Basics of Bayesian linear regression 1.1 Bayes’ theorem Theorem 1.1 (Bayes' theorem) For events \\(A, B\\) and \\(P(B) \\neq 0\\), we have \\[P(A\\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}\\] We denote \\(U\\) as unknown parameters and \\(K\\) as known parameters. We call \\(P(U)\\) prior and \\(P(K|U)\\) likelihood. The Bayes’ theorem gives us the posterior distribution of unknown parameters given the known parameters \\[ P(U \\mid K) \\propto P(U) \\cdot P(K \\mid U)\\] Let \\(K = \\left\\{y_{n \\times 1}, X_{n \\times p} \\right\\}\\) and assume \\(y \\sim N\\left( X \\beta, \\sigma^{2} V\\right)\\), where \\(V\\) is known and \\(U = \\left\\{\\beta, \\sigma^{2}\\right\\}\\) is unknown. The likelihood is given by \\[\\begin{align} P(K \\mid U)=N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\end{align}\\] 1.2 Normal Inverse-Gamma (NIG) prior 1.2.1 Joint distribution of NIG prior Definition 1.1 (Normal Inverse-Gamma Distribution) Suppose \\[ \\begin{aligned} &amp; x \\mid \\sigma^2, \\mu, M \\sim \\text{N}(\\mu,\\sigma^2 M) \\\\ &amp; \\sigma^2 \\mid \\alpha, \\beta \\sim \\text{IG}(\\alpha,\\beta) \\end{aligned} \\] Then \\((x,\\sigma^2)\\) has a Normal-Inverse-Gamma distribution, denoted as \\((x,\\sigma^2) \\sim \\text{NIG}(\\mu,M,\\alpha,\\beta)\\). We use a Normal Inverse-Gamma prior for \\((\\beta, \\sigma^2)\\) \\[\\begin{align} P(\\beta, \\sigma^{2}) &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi)^{\\frac{p}{2}}\\left|\\sigma^{2} M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\\\ \\end{align}\\] where \\(Q(x, m, M)=(x-m)^{\\top} M^{-1} (x-m)\\) Note: the Inverse-Gamma (IG) distribution has a relationship with Gamma distribution. \\(X \\sim Gamma(\\alpha, \\beta)\\), the density function of \\(X\\) is \\(f(x)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha-1} e^{-\\beta x}\\). Let \\(Y=\\frac{1}{X} \\sim IG(\\alpha, \\beta)\\), the density function of \\(Y\\) is \\(f(y)=\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{-\\alpha-1} e^{-\\frac{\\beta}{x}}\\). 1.2.2 Marginal distribution of NIG prior As for marginal priors, we can can get it by integration \\[ \\begin{aligned} P(\\sigma^2) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta=I G\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\\\ P(\\beta) &amp; = \\int N I G\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2}=t_{2a_0}(m_0, \\frac{b_0}{a_0}M_0) \\\\ \\end{aligned} \\] Click to show or hide details \\[\\begin{align} P\\left(\\sigma^{2} \\right) &amp;= \\int NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\int N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\ d\\beta \\\\ &amp;=IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\end{align}\\] \\[\\begin{align} P(\\beta ) &amp;=\\int NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\ d\\sigma^{2} \\\\ &amp;= \\int \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\ d\\sigma^{2} \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^{2}}(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right))} \\ d\\sigma^{2} \\\\ &amp; \\quad (\\text{let } u = \\frac{1}{\\sigma^2}, \\left|d\\sigma^{2}\\right|=\\frac{1}{u^2} d u) \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}+1} e^{-(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)) u } \\frac{1}{u^2} \\ du \\\\ &amp;= \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} \\int u^{a_{0}+\\frac{p}{2}-1} e^{-(b_{0}+\\frac{1}{2} Q \\left(\\beta, m_{0}, M_{0}\\right)) u} \\ du \\\\ &amp; \\quad (\\text{by Gamma integral function:} \\int x^{\\alpha - 1} exp^{-\\beta x} dx = \\frac{\\Gamma(\\alpha)}{\\beta^{\\alpha}}) \\\\ &amp;= \\frac{b_{0}^{a_{0}} }{\\Gamma\\left(a_{0}\\right)(2 \\pi)^\\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left[b_{0}+\\frac{1}{2} Q(\\beta,m_0,M_0)\\right]^{\\left(a_{0}+\\frac{p}{2}\\right)}} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left[b_0(1+\\frac{1}{2 b_0} Q(\\beta,m_0,M_0))\\right]^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; = \\frac{b_0^{a_0}\\Gamma\\left(a_{0}+\\frac{p}{2}\\right) b_0^{- \\left( a_0+\\frac{p}{2}\\right)}}{\\Gamma\\left(a_{0}\\right)(2 \\pi)^ \\frac{p}{2}\\left|M_{0}\\right|^{\\frac{1}{2}}} \\left[1+\\frac{1}{2 b_0} \\left(\\beta-m_{0}\\right){\\top} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right]^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 \\pi \\right)^{\\frac{p}{2}} b_{0}^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}}\\left[1+\\frac{1}{2 b_{0}} \\left(\\beta-m_{0}\\right){\\top} M_{0}^{-1}\\left(\\beta-m_{0}\\right) \\right]^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)} {\\left(2 \\pi \\right)^{\\frac{p}{2}}\\left(a_{0} \\cdot \\frac{b_{0}}{a_{0}}\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)|M|^{\\frac{1}{2}}} \\left[1+\\frac{1}{2 a_{0} \\cdot \\frac{b_{0}}{a_{0}}} \\left(\\beta-m_{0}\\right){\\top} M_{0}^{-1}\\left(\\beta-m_{0}\\right)\\right]^{-\\left(a_{0}+\\frac{p}{2}\\right)}\\\\ &amp; =\\frac{\\Gamma\\left(a_{0}+\\frac{p}{2}\\right)}{\\left(2 a_{0} \\pi\\right)^{\\frac{p}{2}} \\Gamma\\left(a_{0}\\right)\\left|\\frac{b_{0}}{a_{0}} M\\right|^{\\frac{1}{2}}}\\left[1+\\frac{1}{2 a_{0}} \\left(\\beta-m_{0}\\right)^{\\top}\\left(\\frac{b_{0}}{a_{0}} M_{0}\\right)^{-1}\\left(\\beta-m_{0}\\right)\\right]^{-\\left(a_{0}+\\frac{p}{2}\\right)} \\\\ &amp; =t_{2a_0}(m_0, \\frac{b_0}{a_0}M_0) \\; \\end{align}\\] Note: the density of multivariate t-distribution is given by \\[ t_v(\\mu, \\Sigma)=\\frac{\\Gamma\\left(\\frac{v+p}{2}\\right)}{(v \\pi)^{\\frac{p}{2}} \\Gamma\\left(\\frac{v}{2}\\right) |\\Sigma|^{\\frac{1}{2}}}\\left[1+\\frac{1}{v}(x-\\mu)^{\\top} \\Sigma^{-1}(x-\\mu)\\right]^{-\\frac{v+p}{2}} \\] 1.3 Posterior distribution The posterior distribution of \\((\\beta, \\sigma^2)\\) is given by \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1} &amp;= (M_{0}^{-1}+X^{\\top} V^{-1} X)^{-1} \\;; \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\top} V^{-1} y \\;; \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\;; \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\top} M_{0}^{-1} m_{0}+y^{\\top} V^{-1} y-m_{1}^{\\top} M_{1} m_{1}\\right)\\;. \\end{align}\\] Click to show or hide details \\[\\begin{align}\\label{eq:post_dist} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto NIG\\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto IG\\left(\\sigma^{2} \\mid a_{0}, b_{0}\\right) \\cdot N\\left(\\beta \\mid m_{0}, \\sigma^{2} M_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} V\\right) \\nonumber\\\\ &amp; \\propto \\frac{b_0^{a_0}}{\\Gamma\\left(a_{0}\\right)} \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| M_{0}\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(\\beta, m_{0}, M_{0}\\right)} \\frac{1}{(2 \\pi \\sigma^{2})^{\\frac{p}{2}}\\left| V\\right|^{\\frac{1}{2}}} e^{-\\frac{1}{2 \\sigma^{2}} Q \\left(y, X \\beta, V\\right)} \\nonumber\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} (Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right))}\\; \\end{align}\\] where \\[\\begin{align}\\label{eq:multivariate_completion_square} Q \\left(\\beta, m_{0}, M_{0}\\right)+Q \\left(y, X \\beta, V\\right) &amp;= (\\beta - m_{0})^{\\top}M_{0}^{-1}(\\beta - m_{0}) + (y - X\\beta)^{\\top}V^{-1}(y - X\\beta)\\; \\nonumber\\\\ &amp;= \\beta^{\\top}M_{0}^{-1}\\beta - 2\\beta^{\\top}M_{0}^{-1}m_{0} + m_{0}^{\\top}M_{0}^{-1}m_{0} \\nonumber\\\\ &amp;\\qquad + \\beta^{\\top}X^{\\top}V^{-1}X\\beta - 2\\beta^{\\top} X^{\\top}V^{-1}y + y^{\\top}V^{-1}y \\nonumber\\\\ &amp;= \\beta^{\\top} \\left(M_{0}^{-1} + X^{\\top}V^{-1}X\\right) \\beta - 2\\beta^{\\top}\\left(M_{0}^{-1}m_{0} + X^{\\top}V^{-1}y\\right) \\nonumber\\\\ &amp;\\qquad + m_{0}^{\\top} M_{0}^{-1}m_{0} + y^{\\top}V^{-1}y \\nonumber \\\\ &amp;= \\beta^{\\top}M_{1}^{-1}\\beta - 2\\beta^{\\top} m_{1} + c\\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\top}M_{1}^{-1}(\\beta - M_{1}m_{1}) - m_{1}^{\\top}M_{1}m_{1} +c \\nonumber\\\\ &amp;= (\\beta - M_{1}m_{1})^{\\top}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast}\\; \\end{align}\\] where \\(M_{1}\\) is a symmetric positive definite matrix, \\(m_{1}\\) is a vector, and \\(c\\) &amp; \\(c^{\\ast}\\) are scalars given by \\[\\begin{align} M_{1} &amp;= (M_{0}^{-1} + X^{\\top}V^{-1}X)^{-1}\\;; \\\\ m_{1} &amp;= M_{0}^{-1}m_{0} + X^{\\top}V^{-1}y\\;; \\\\ c &amp;= m_{0}^{\\top} M_{0}^{-1}m_{0} + y^{\\top}V^{-1}y\\;; \\\\ c^{\\ast} &amp;= c - m^{\\top}Mm = m_{0}^{\\top} M_{0}^{-1}m_{0} + y^{\\top}V^{-1}y - m_{1}^{\\top}M_{1}m_{1}\\; . \\end{align}\\] Note: \\(M_{1}\\), \\(m_{1}\\) and \\(c\\) do not depend upon \\(\\beta\\). Then, we have \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} ((\\beta - M_{1}m_{1})^{\\top}M_{1}^{-1}(\\beta - M_{1}m_{1}) +c^{\\ast})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+p+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\top}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp; \\propto \\left(\\frac{1}{\\sigma^{2}}\\right)^{a_{0}+\\frac{p}{2}+1} e^{-\\frac{b_{0}+\\frac{c^{\\ast}}{2}}{\\sigma^{2}}} (\\frac{1}{\\sigma^2})^{\\frac{p}{2}} e^{-\\frac{1}{2 \\sigma^{2}} (\\beta - M_{1}m_{1})^{\\top}M_{1}^{-1}(\\beta - M_{1}m_{1})}\\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{0}+\\frac{p}{2}, b_{0}+\\frac{c^{\\ast}}{2} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp;= IG\\left(\\sigma^{2} \\mid a_{1}, b_{1} \\right) \\cdot N\\left(\\beta \\mid M_{1}m_{1}, \\sigma^{2} M_{1}\\right) \\\\ &amp;= NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1} &amp;= (M_{0}^{-1}+X^{\\top} V^{-1} X)^{-1} \\;; \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\top} V^{-1} y \\;; \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\;; \\\\ b_{1}&amp;=b_{0}+\\frac{c^{\\ast}}{2}= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\top} M_{0}^{-1} m_{0}+y^{\\top} V^{-1} y-m_{1}^{\\top} M_{1} m_{1}\\right)\\;. \\end{align}\\] From derivation in marginal priors, the marginal posterior distributions can be easily get by updating corresponding parameters \\[ \\begin{aligned} &amp;P\\left(\\sigma^{2} \\mid y\\right)=I G\\left(\\sigma^{2} \\mid a_{1}, b_{1}\\right) \\\\ &amp;P(\\beta \\mid y)=t_{2a_1}(M_1m_1, \\frac{b_1}{a_1}M_1) \\end{aligned} \\] 1.4 Bayesian prediction Assume \\(V=I_{n}\\). Let \\(\\tilde{y}\\) denote an \\(\\tilde{n}\\times 1\\) vector of outcomes. \\(\\tilde{X}\\) is corresponding predictors. We seek to predict \\(\\tilde{y}\\) based upon \\(y\\) \\[\\begin{align} P(\\tilde{y} \\mid y) &amp;= t_{2a_1}(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\top})) \\; \\end{align}\\] Click to show or hide details \\[\\begin{align} P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) &amp;=P\\left(\\beta, \\sigma^{2} \\mid y\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp; \\propto P\\left(\\beta, \\sigma^{2}\\right) \\cdot P\\left(y \\mid \\beta, \\sigma^{2}\\right) \\cdot P\\left(\\tilde{y} \\mid \\beta, \\sigma^{2}, y\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right) \\cdot N\\left(y \\mid X \\beta, \\sigma^{2} I_{n}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= NIG \\left(\\beta, \\sigma^{2} \\mid M_{1} m_{1}, M_{1}, a_{1}, b_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\\\ &amp;= IG(\\sigma^{2} \\mid a_{1}, b_{1}) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}} \\right) \\; \\end{align}\\] Then we can calculate posterior predictive density \\(P(\\tilde{y} \\mid y)\\) from \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) \\[\\begin{align} P(\\tilde{y} \\mid y) &amp;=\\iint P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\iint IG(\\sigma^{2} \\mid a_{1}, b_{1}) \\cdot N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ &amp;=\\int IG(\\sigma^{2} \\mid a_{1}, b_{1}) \\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1} \\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta \\ d\\sigma^{2} \\\\ \\end{align}\\] As for \\(\\int N\\left(\\beta \\mid M_{1} m_{1}, \\sigma^{2} M_{1}\\right) \\cdot N\\left(\\tilde{y} \\mid \\tilde{X} \\beta, \\sigma^{2} I_{\\tilde{n}}\\right) \\ d\\beta\\), we provide an easy way to derive it avoiding any integration at all. Note that we can write the above model as \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} \\beta + \\tilde{\\epsilon}, \\text{ where } \\tilde{\\epsilon} \\sim N(0,\\sigma^2 I_{\\tilde{n}}) \\\\ \\beta &amp;= M_{1} m_{1} + \\epsilon_{\\beta \\mid y}, \\text{ where } \\epsilon_{\\beta \\mid y} \\sim N(0,\\sigma^2M_{1}) \\end{align}\\] where \\(\\tilde{\\epsilon}\\) and \\(\\epsilon_{\\beta \\mid y}\\) are independent of each other. It then follows that \\[\\begin{align} \\tilde{y} &amp;= \\tilde{X} M_{1} m_{1} + \\tilde{X} \\epsilon_{\\beta \\mid y} + \\tilde{\\epsilon} \\sim N(\\tilde{X} M_{1} m_{1}, \\sigma^2(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\top})) \\end{align}\\] As a result \\[\\begin{align} P(\\tilde{y} \\mid y) &amp;=\\int IG(\\sigma^{2} \\mid a_{1}, b_{1}) \\cdot N(\\tilde{X} M_{1} m_{1}, \\sigma^2(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\top})) \\ d\\sigma^{2} \\\\ &amp;= t_{2a_1}(\\tilde{X} M_1 m_1, \\frac{b_1}{a_1}(I_{\\tilde{n}} + \\tilde{X} M_{1} \\tilde{X}^{\\top})) \\; \\end{align}\\] 1.5 Sampling process We can get the joint posterior density \\(P\\left(\\beta, \\sigma^{2}, \\tilde{y} \\mid y\\right)\\) by sampling process Draw \\(\\hat{\\sigma}_{(i)}^{2}\\) from \\(I G\\left(a_{1}, b_{1}\\right)\\) Draw \\(\\hat{\\beta}_{(i)}\\) from \\(N\\left(M_{1} m_{1}, \\hat{\\sigma}_{(i)}^{2} M_{1}\\right)\\) Draw \\(\\tilde{y}_{(i)}\\) from \\(N\\left(\\tilde{X} \\hat{\\beta}_{(i)}, \\hat{\\sigma}_{(i)}^{2}I_{\\tilde{n}}\\right)\\) "],["updating-form-of-the-posterior-distribution.html", "Chapter 2 Updating form of the posterior distribution 2.1 Method 1: Sherman-Woodbury-Morrison identity 2.2 Method 2: distribution theory", " Chapter 2 Updating form of the posterior distribution Assume \\(y \\sim N\\left( X \\beta, \\sigma^{2} V\\right)\\) and \\(P(\\beta, \\sigma^{2}) = NIG \\left(\\beta, \\sigma^{2} \\mid m_{0}, M_{0}, a_{0}, b_{0}\\right)\\), the posterior distribution is given by \\[\\begin{align} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid M_{1}m_{1}, M_{1}, a_{1}, b_{1}\\right) \\; \\end{align}\\] where \\[\\begin{align} M_{1} &amp;= (M_{0}^{-1}+X^{\\top} V^{-1} X)^{-1} \\;; \\\\ m_{1}&amp;=M_{0}^{-1} m_{0}+X^{\\top} V^{-1} y \\;; \\\\ a_{1}&amp;=a_{0}+\\frac{p}{2} \\;; \\\\ b_{1}&amp;= b_{0}+\\frac{1}{2}\\left(m_{0}^{\\top} M_{0}^{-1} m_{0}+y^{\\top} V^{-1} y-m_{1}^{\\top} M_{1} m_{1}\\right)\\;. \\end{align}\\] We will use two ways to calculate \\(M_1\\). 2.1 Method 1: Sherman-Woodbury-Morrison identity To calculate \\(M_1\\), we can utilize the well-known Sherman-Woodbury-Morrison identity in matrix algebra \\[\\begin{equation}\\label{ShermanWoodburyMorrison} \\left(A + BDC\\right)^{-1} = A^{-1} - A^{-1}B\\left(D^{-1}+CA^{-1}B\\right)^{-1}CA^{-1} \\end{equation}\\] where \\(A\\) and \\(D\\) are square matrices that are invertible and \\(B\\) and \\(C\\) are rectangular (square if \\(A\\) and \\(D\\) have the same dimensions) matrices such that the multiplications are well-defined. This identity is easily verified by multiplying the right hand side with \\(A + BDC\\) and simplifying to reduce it to the identity matrix. \\[ \\begin{aligned} M_1 &amp; = (M_{0}^{-1} + X^{\\top}V^{-1}X)^{-1} \\\\ &amp; = M_0-M_0 X^{\\top}\\left(V+X M_0 X^{\\top}\\right)^{-1} X M_0 \\\\ &amp; = M_0-M_0 X^{\\top} Q^{-1} X M_0 \\end{aligned} \\] where \\(Q = V + X M_0 X^{\\top}\\) We can show that \\[ \\begin{align} M_1 m_1 &amp; =m_0+M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\;. \\end{align} \\] Click to show or hide details \\[\\begin{align} M_1 m_1 &amp; = \\left(M_0^{-1}+X^{\\top} V^{-1} X\\right)^{-1} m_1 \\\\ &amp; = [M_0-M_0 X^{\\top}\\left(V+X M_0 X^{\\top}\\right)^{-1} X M_0]m_1 \\\\ &amp; = (M_0-M_0 X^{\\top} Q^{-1} X M_0) m_1 \\\\ &amp; = (M_0-M_0 X^{\\top} Q^{-1} X M_0)(M_0^{-1} m_0+X^{\\top} V^{-1} y) \\\\ &amp; = m_0+M_0 X^{\\top} V^{-1} y-M_0 X^{\\top} Q^{-1} X m_0 - M_0 X^{\\top} Q^{-1} X M_0 X^{\\top} V^{-1} y \\\\ &amp; = m_0+M_0 X^{\\top}\\left(I-Q^{-1} X M_0 X^{\\top}\\right) V^{-1} y - M_0 X^{\\top} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\top} Q^{-1}\\left(Q-X M_0 X^{\\top}\\right)V^{-1} y - M_0 X^{\\top} Q^{-1} X m_0 \\\\ &amp; \\left(\\text { since } Q=V+X M_0 X^{\\top}\\right) \\\\ &amp; = m_0+M_0 X^{\\top} Q^{-1}(V) V^{-1} y-M_0 X^{\\top} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\top} Q^{-1} y-M_0 X^{\\top} Q^{-1} X m_0 \\\\ &amp; = m_0+M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] Furthermore, we can simplify that \\[ \\begin{align} m_0^{\\top} M_0^{-1} m_0+y^{\\top} V^{-1} y-m_1^{\\top} M_1 m_1 &amp; = \\left(y-X m_0\\right)^{\\top} Q^{-1}\\left(y-X m_0\\right) \\;. \\end{align} \\] Click to show or hide details \\[\\begin{align} m_0^{\\top} M_0^{-1} m_0+y^{\\top} V^{-1} y-m_1^{\\top} M_1 m_1 &amp; = m_0^{\\top} M_0^{-1} m_0+y^{\\top} V^{-1} y-m_1^{\\top} [m_0+M_0 X^{\\top} Q^{-1} (y - X m_0)] \\\\ &amp; = m_0^{\\top} M_0^{-1} m_0+y^{\\top} V^{-1} y-m_1^{\\top} m_0 - m_1^{\\top} M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = m_0^{\\top} M_0^{-1} m_0+y^{\\top} V^{-1} y -m_0^{\\top}\\left(M_0^{-1} m_0+X^{\\top} V^{-1} y\\right) \\\\ &amp; \\qquad \\qquad \\qquad - m_1^{\\top} M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\top} V^{-1} y-y^{\\top} V^{-1} X m_0 - m_1^{\\top} M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; = y^{\\top} V^{-1}\\left(y-X m_0 \\right)-m_1^{\\top} M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-\\underbrace{m_1^{\\top} M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right)}_{\\substack{\\text { simplify from left to right }}} \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-\\left(M_0 m_1\\right)^{\\top} X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-\\left(m_0+M_0 X^{\\top} V^{-1} y\\right)^{\\top} X^{\\top} Q^{-1}\\left(y-m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-\\left(X m_0+X M_0 X^{\\top} V^{-1} y\\right)^{\\top} Q^{-1}\\left(y-X m_0\\right)\\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right) -\\left(Q^{-1} X m_0+Q^{-1}\\left(X M_0 X^{\\top}\\right)V^{-1} y\\right)\\left(y-X m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-[Q^{-1} X m_0+Q^{-1}(Q-V) V^{-1} y]^{\\top}(y-X m_0) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right) -\\left(Q^{-1} X m_0+V^{-1} y- Q^{-1} y \\right)^{\\top}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right) -[V^{-1} y+Q^{-1}\\left(X m_0-y\\right)]^{\\top}\\left(y-X m_0\\right) \\\\ &amp; =y^{\\top} V^{-1}\\left(y-X m_0\\right)-y^{\\top} V^{-1}\\left(y-X m_0\\right) +\\left(y-X m_0\\right)^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ &amp; =\\left(y-X m_0\\right)^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ \\end{align}\\] So, we get the following updating form of the posterior distribution from Bayesian linear regression \\[ \\begin{aligned} P\\left(\\beta, \\sigma^{2} \\mid y\\right) = NIG\\left(\\beta, \\sigma^{2} \\mid \\tilde{m}_1, \\tilde{M}_1, a_{1}, b_{1}\\right) \\; \\end{aligned} \\] where \\[ \\begin{aligned} \\tilde{m}_1 &amp; =M_1 m_1=m_0+M_0 X^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ \\tilde{M}_1 &amp; =M_1=M_0-M_0 X^{\\top} Q^{-1} X M_0 \\\\ a_1 &amp; =a_0+\\frac{p}{2} \\\\ b_1 &amp; =b_0+\\frac{1}{2}\\left(y-X m_0\\right)^{\\top} Q^{-1}\\left(y-X m_0\\right) \\\\ Q &amp; =V+X M_0 X^{\\top} \\end{aligned} \\] 2.2 Method 2: distribution theory Previously, we got the Bayesian Linear Regression Updater using Sherman-Woodbury-Morrison identity. Here, we will derive the results without resorting to it. The model is given by \\[\\begin{align} &amp; y=X \\beta+\\epsilon , \\quad \\epsilon \\sim N\\left(0, \\sigma^2 V\\right) ; \\\\ &amp; \\beta=m_0+\\omega , \\quad \\omega \\sim N\\left(0, \\sigma^2 M_0\\right) ; \\\\ &amp; \\sigma^2 \\sim I G\\left(a_0, b_0\\right) \\;. \\end{align}\\] This corresponds to the posterior distribution \\[\\begin{align} P\\left(\\beta, \\sigma^2 \\mid y\\right) \\propto I G\\left(\\sigma^2 \\mid a_0, b_0\\right) &amp; \\times N\\left(\\beta \\mid m_0, \\sigma^2 M_0\\right) \\times N\\left(y \\mid X \\beta, \\sigma^2 V\\right) \\;. \\end{align}\\] We will derive \\(P\\left(\\sigma^2 \\mid y\\right)\\) and \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\) in a form that will reflect updates from the prior to the posterior. Integrating out \\(\\beta\\) from the model is equivalent to substituting \\(\\beta\\) from its prior model. Thus, \\(P\\left(y \\mid \\sigma^2\\right)\\) is derived simply from \\[\\begin{align} y &amp;=X \\beta+\\epsilon \\\\ &amp;=X\\left(m_0+\\omega\\right)+\\epsilon \\\\ &amp;=X m_0 + X \\omega + \\epsilon \\\\ &amp; =X m_0+ \\eta \\;, \\\\ \\end{align}\\] where \\[\\begin{align} &amp; \\eta = X \\omega + \\epsilon \\sim N\\left(0, \\sigma^2Q\\right) \\; ; \\\\ &amp; Q=X M_0 X^{\\top}+V \\; . \\\\ \\end{align}\\] Therefore, \\[\\begin{align} y \\mid \\sigma^2 \\sim N\\left(X m_0, \\sigma^2 Q\\right) \\; .\\\\ \\end{align}\\] The posterior distribution is given by: \\[\\begin{align} P\\left(\\sigma^2 \\mid y\\right) &amp; \\propto P\\left(\\sigma^2\\right) P\\left(y \\mid \\sigma^2\\right) \\\\ &amp; =I G\\left(\\sigma^2 \\mid a_0, b_0\\right) \\times N\\left(y \\mid X m_0, \\sigma^2 Q\\right) \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+1} e^{-\\frac{b_0} {\\sigma^2} \\times\\left(\\frac{1}{\\sigma^2}\\right)^{\\frac{n}{2}} e^{-\\frac{1}{2 \\sigma^2}}\\left(y-Xm_0\\right)^{\\top} Q^{-1}\\left(y-Xm_0\\right)} \\\\ &amp; \\propto\\left(\\frac{1}{\\sigma^2}\\right)^{a_0+\\frac{p}{2}+1} e^{-\\frac{1}{\\sigma^2}\\left\\{b_0+\\frac{1}{2}\\left(y-Xm_0\\right)^{\\top} Q^{-1}\\left(y-Xm_0\\right)\\right.} \\\\ &amp; \\propto IG \\left(\\sigma^2 \\mid a_1, b_1\\right) \\; , \\end{align}\\] where \\[\\begin{align} &amp; a_1 = a_0 + \\frac{p}{2} ; \\\\ &amp; b_1 = b_0 + \\frac{1}{2} (y-Xm_0)^{\\top} Q^{-1} \\left(y-Xm_0\\right) \\; . \\end{align}\\] Next, we turn to \\(P\\left(\\beta \\mid \\sigma^2, y\\right)\\). Note that \\[ \\left[\\begin{array}{l} y \\\\ \\beta \\end{array}\\right] \\mid \\sigma^2 \\sim N\\left(\\left[\\begin{array}{l} Xm_0 \\\\ m_0 \\end{array}\\right], \\quad \\sigma^2 \\left[\\begin{array}{cc} Q &amp; X M_0 \\\\ M_0 X^{\\top} &amp; M_0 \\end{array}\\right]\\right) \\; . \\] Click to show or hide details where we have used the facts \\[ \\begin{aligned} &amp; E[y \\mid \\sigma^2] = Xm_0 \\; ;\\\\ &amp; E[\\beta \\mid \\sigma^2] = m_0 \\; ; \\\\ &amp; \\operatorname{Var}\\left(y \\mid \\sigma^2\\right)=\\sigma^2 Q \\; ; \\\\ &amp; \\operatorname{Var}\\left(\\beta \\mid \\sigma^2\\right)=\\sigma^2 M_0 \\; ; \\\\ \\end{aligned} \\] \\[ \\begin{aligned} \\operatorname{Cov}\\left(y, \\beta \\mid \\sigma^2\\right) &amp;= \\operatorname{Cov}\\left(X \\beta+\\epsilon, \\beta \\mid \\sigma^2\\right) \\\\ &amp; =\\operatorname{Cov}\\left(X\\left(m_0+\\omega\\right)+\\epsilon, m_0+\\omega \\mid \\sigma^2\\right) \\\\ &amp; =\\operatorname{Cov}\\left(X \\omega, \\omega \\mid \\sigma^2\\right) \\\\ &amp; \\quad \\text {(Since } m_0 \\text { is constant and } \\operatorname{Cov}(\\omega, \\epsilon)=0) \\\\ &amp; =\\sigma^2 X M_0 \\; . \\end{aligned} \\] From the expression of a conditional distribution derived from a multivariate Gaussian, we obtain \\[ \\beta \\mid \\sigma^2, y \\sim N\\left(\\tilde{m}_1, \\sigma^2 \\tilde{M}_1\\right) \\;, \\] where \\[\\begin{align} &amp; \\tilde{m}_1=E\\left[\\beta \\mid \\sigma^2, y\\right]=m_0+M_0 X^{\\top} Q^{-1}\\left(y-X{m_0}\\right) \\; ;\\\\ &amp; \\tilde{M}_1=M_0-M_0 X^{\\top} Q^{-1} X M_0 \\; . \\\\ \\end{align}\\] Click to show or hide details Note: \\[\\begin{align} &amp; \\left[\\begin{array}{l} X_1 \\\\ X_2 \\end{array}\\right] \\sim N\\left(\\left[\\begin{array}{l} \\mu_1 \\\\ \\mu_2 \\end{array}\\right],\\left[\\begin{array}{ll} \\Sigma_{11} &amp; \\Sigma_{12} \\\\ \\Sigma_{21} &amp; \\Sigma_{22} \\end{array}\\right]\\right) \\text { with } \\Sigma_{21} = \\Sigma_{12}^{\\top} \\;, \\\\ &amp; \\Rightarrow X_2 \\mid X_1 \\sim N\\left(\\mu_{2 \\cdot 1}, \\Sigma_{2 \\cdot 1}\\right) \\;, \\\\ &amp; \\text {where } \\mu_{2 \\cdot 1}= \\mu_2+\\Sigma_{21} \\Sigma_{11}^{-1}\\left(X_1-\\mu_1\\right) \\text { and } \\Sigma_{2 \\cdot 1}=\\Sigma_{22}-\\Sigma_{21} \\Sigma_{11}^{-1} \\Sigma_{12} \\;. \\end{align}\\] "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
